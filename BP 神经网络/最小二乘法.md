以下是 **最小二乘法权重偏导推导详解（修正原图错误）** 的完整 Markdown 版本，包含公式推导、错误修正、链式法则应用、具体实例验证及代码实现。

---

# 最小二乘法权重偏导推导详解（修正原图错误）

以下是基于用户图片内容的详细推导过程，修正公式排版错误并分步解释原理。

---

## 一、修正原图中的关键错误

1. **损失函数多项式错误**
   原图中出现 $w_2x^{-2}$ 是错误的，应为 $w_j(x^{(i)})^j$，表示多项式模型的正幂次项。

2. **偏导数符号混乱**
   比如 $\frac{a}{b u_2}$ 应是 $\frac{\partial J}{\partial w_j}$，属于排版问题。

3. **求和项缺失**
   某些中间步骤缺少 $\sum_{i=1}^m$ 符号，应补全以表示所有样本求和。

---

## 二、推导步骤与原理

### 1. 定义损失函数（均方误差）

$$
J = \frac{1}{2m} \sum_{i=1}^m \left( f(x^{(i)}) - y^{(i)} \right)^2
$$

其中：

* $f(x^{(i)}) = w_0 + w_1x^{(i)} + w_2(x^{(i)})^2 + \dots + w_n(x^{(i)})^n$
* $m$：样本数

---

### 2. 对 $w_j$ 求偏导

目标：找到使损失函数 $J$ 最小的 $w_j$，需令导数为零。

#### (1) 展开损失函数

$$
J = \frac{1}{2m} \sum_{i=1}^m \left[ \sum_{k=0}^n w_k(x^{(i)})^k - y^{(i)} \right]^2
$$

#### (2) 应用链式法则

$$
\frac{\partial J}{\partial w_j} = \frac{1}{2m} \sum_{i=1}^m 2 \left( f(x^{(i)}) - y^{(i)} \right) \cdot \frac{\partial f(x^{(i)})}{\partial w_j}
$$

其中：

$$
\frac{\partial f(x^{(i)})}{\partial w_j} = (x^{(i)})^j
$$

#### (3) 化简表达式

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^m \left( f(x^{(i)}) - y^{(i)} \right) \cdot (x^{(i)})^j
$$

---

## 三、正规方程推导

对每个权重 $w_j$，令偏导为零：

$$
\sum_{i=1}^m \left( f(x^{(i)}) - y^{(i)} \right) \cdot (x^{(i)})^j = 0 \quad (j = 0, 1, \dots, n)
$$

* $j = 0$：意味着残差之和为零（残差均值为零）。
* $j > 0$：意味着残差与特征项 $(x^{(i)})^j$ 的协方差为零。

---

## 四、一元线性回归案例验证

### 1. 数据样本

$$
\{(1,2), (2,3), (3,4)\}
$$

模型形式：

$$
f(x) = w_0 + w_1x
$$

### 2. 偏导公式

* 对 $w_0$：

$$
\frac{\partial J}{\partial w_0} = \frac{1}{m} \sum_{i=1}^m (w_0 + w_1x^{(i)} - y^{(i)})
$$

* 对 $w_1$：

$$
\frac{\partial J}{\partial w_1} = \frac{1}{m} \sum_{i=1}^m (w_0 + w_1x^{(i)} - y^{(i)}) \cdot x^{(i)}
$$

### 3. 解正规方程

代入样本点 $x = [1,2,3]$，$y = [2,3,4]$，解得：

$$
w_0 = 1, \quad w_1 = 1
$$

拟合直线：

$$
f(x) = 1 + x
$$

---

## 五、几何意义与代码实现

### 1. 几何意义

最小二乘法的本质是 **将观测向量 $\mathbf{y}$ 正交投影到特征矩阵 $\mathbf{X}$ 的列空间**，使得残差矢量 $\mathbf{e} = \mathbf{y} - \mathbf{Xw}$ 与列空间正交。

### 2. Python 实现

```python
import numpy as np

# 样本数据
X = np.array([[1, 1], [1, 2], [1, 3]])  # 加上截距项列
y = np.array([2, 3, 4])

# 正规方程解
w = np.linalg.inv(X.T @ X) @ X.T @ y
print(f"权重: w0 = {w[0]:.1f}, w1 = {w[1]:.1f}")
```

**输出：**

```
权重: w0 = 1.0, w1 = 1.0
```

---

## 六、总结

* **核心导数公式**：

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^m (f(x^{(i)}) - y^{(i)}) \cdot (x^{(i)})^j
$$

* **推导思路**：

  * 利用链式法则将复合函数导数拆解。
  * 聚焦于当前权重对应的特征项。
* **应用意义**：

  * 通过正规方程解析解或梯度下降迭代求解权重。
  * 铺垫神经网络反向传播、多项式拟合等机器学习基础。

---

如需添加图示帮助理解几何意义，我可以为你生成一张图像，你是否需要？
